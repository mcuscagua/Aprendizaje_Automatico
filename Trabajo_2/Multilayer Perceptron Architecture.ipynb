{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, precision_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import dropbox\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "standarscaler = StandardScaler()\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('https://raw.githubusercontent.com/mcuscagua/Aprendizaje_Automatico/master/Trabajo_2/Data.csv')\n",
    "Data = Data.set_index('Date')\n",
    "Y = Data['Class']\n",
    "X = Data.drop('Class', axis = 1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "X_train_Stand = standarscaler.fit_transform(X_train)\n",
    "X_test_Stand = standarscaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neurons = [x+1 for x in range(10)]\n",
    "all_combinations = []\n",
    "for i in range(2):\n",
    "    all_combinations += [p for p in it.product(Neurons, repeat=i+1)]\n",
    "    \n",
    "TSZ_LC_Vector = np.ndarray((5,len(all_combinations)))\n",
    "TrSCr_LC_Vector = np.ndarray((5,len(all_combinations)))\n",
    "TeSCr_LC_Vector = np.ndarray((5,len(all_combinations)))\n",
    "AUC_vector = np.repeat(0,len(all_combinations))\n",
    "Accuracy_vector = np.repeat(0,len(all_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_combinations)):\n",
    "    MLPC = MLPClassifier(hidden_layer_sizes=all_combinations[i],\n",
    "                             learning_rate = 'constant',\n",
    "                             learning_rate_init = 0.2,\n",
    "                             max_iter = 50,\n",
    "                             tol = 1e-2,\n",
    "                             random_state=0)\n",
    "\n",
    "    MLPP = MLPClassifier(hidden_layer_sizes=all_combinations[i],\n",
    "                             learning_rate = 'constant',\n",
    "                             learning_rate_init = 0.2,\n",
    "                             max_iter = 50,\n",
    "                             tol = 1e-2,\n",
    "                             random_state=0)\n",
    "\n",
    "    MLPC.fit(X_train_Stand, Y_train)\n",
    "    y_score = MLPC.predict_proba(X_test_Stand)\n",
    "    y_pred = MLPC.predict(X_test_Stand)\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(MLPP, X, Y)\n",
    "    \n",
    "    TSZ_LC_Vector[:,i] = train_sizes\n",
    "    TrSCr_LC_Vector[:,i] = np.median(train_scores, axis=1)\n",
    "    TeSCr_LC_Vector[:,i] = np.median(test_scores, axis=1)\n",
    "    AUC_vector[i] = roc_auc_score(Y_test, y_pred)\n",
    "    Accuracy_vector[i] = accuracy_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,  330.,\n",
       "         330.,  330.],\n",
       "       [1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073., 1073.,\n",
       "        1073., 1073.],\n",
       "       [1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817., 1817.,\n",
       "        1817., 1817.],\n",
       "       [2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560., 2560.,\n",
       "        2560., 2560.],\n",
       "       [3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304., 3304.,\n",
       "        3304., 3304.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC_vector[i] = roc_auc_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5052910052910053\n"
     ]
    }
   ],
   "source": [
    "MLPC = MLPClassifier(hidden_layer_sizes=(10),\n",
    "                         learning_rate = 'constant',\n",
    "                         learning_rate_init = 0.2,\n",
    "                         max_iter = 50,\n",
    "                         tol = 1e-2,\n",
    "                         random_state=0)\n",
    "\n",
    "MLPC.fit(X_train_Stand, Y_train)\n",
    "y_score = MLPC.predict_proba(X_test_Stand)\n",
    "y_pred = MLPC.predict(X_test_Stand)\n",
    "\n",
    "AUC = roc_auc_score(Y_test, y_pred)\n",
    "Acc = accuracy_score(Y_test, y_pred)\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig('The one.png')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja = plot_learning_curve(MLPP, 'LEARNING CURVE', X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja.savefig('The one.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
